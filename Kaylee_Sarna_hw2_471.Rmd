---
title: "471 Hw2"
author: "Kaylee Sarna"
date: "February 27, 2018"
output:
  html_document:
    toc: true
    df_print: paged
    code_folding: show
    keep_md: true
---

##Set-up

```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(comment = NA)

# insert other packages you need here

library(tableone)
library(skimr)
library(forcats)
library(leaps)
library(broom)
library(modelr)
library(ISLR)
library(base)
library(knitr)
library(magrittr)
library(pwr)
library(vcd)
library(gridExtra)
library(mosaic)
library(car)
library(MASS)
library(leaps)
library(tidyverse)


```

#Chapter 5 Exercises

9.
```{r load_boston}
library(MASS)
Boston=Boston
write.csv(Boston, "C:/Users/kaylee/Dropbox/stat 471/Boston.csv")
summary(Boston)

```

9.a. The estimate for the population mean of medv(median value of owner-occupied homes) is about 22.53(in $1000s).
```{r ch_5_q_9_a}
#Estimate mean of medv for population(do this to Boston dataset)
mean(Boston$medv)
```

9.b. The estimate of the standard error is about 0.409.The larger this number is the more likely the accuracy of the predictions is not good. This number is fairly small so more likely accurate.
```{r ch_5_q_9_b}
#We divide standard deviation of Boston medv by square root of observations(506) in Boston dataset
attach(Boston)
sd(medv)/sqrt(length(medv))
```

9.c. The standard error of the sample mean using bootstrap is about 0.412. This is pretty close to the value of 0.409 but is a little higher.
```{r ch_5_q_9_c}
#Use bootstrap to estimate standard error of the sample mean.
set.seed(1)
boot.fn = function(data, index)
 + return(mean(data[index]))
library(boot)
boot(medv, boot.fn, 1000)
```

9.d. The results of the C.I. for the t.test and the bootstrap are basically very similar. 21.71 to 21.73 for bootstrap and t.test respectively. 23.36 to 23.34 for bootstrap and t.test respectively.
```{r ch_5_q_9_d}
#95% confidence interval for mean of medv
mean(medv) - 2 * 0.412
mean(medv) + 2 * 0.412
#Results are 21.71,23.36
#t.test for medv
t.test(medv)
```

9.e.The median value estimate for the population is 21.2.
```{r ch_5_q_9_e}
#Median value of medv in population
median(medv)
```

9.f.The median is 21.2 when using bootstrap. The standard error is about 0.38.This is fairly small.
```{r ch_5_q_9_f}
#standard error of the median using bootstrap
set.seed(1)
boot.fn = function(data, index) return(median(data[index]))
boot(medv, boot.fn, 1000)
```

9.g.An estimate for the 10th percentile of medv is 12.75.
```{r ch_5_q_9_g}
#10th percentile of median medv
quantile(medv, c(0.1))

```

9.h.The bootstrap estimates the 10th percentile to be 12.75 with a standard error of 0.505. This standard error is fairly small compared to the percentile value.
```{r ch_5_q_9_h}
#Bootstrap to estimate standard error of 10th percentile
set.seed(1)
boot.fn=function(data, index) return(quantile(data[index], c(0.1)))
boot(medv, boot.fn, 1000)
```


#Chapter 6 exercises

9.
```{r data}
library(MASS)
College = College
write.csv(College, "C:/Users/kaylee/Dropbox/stat 471/College.csv")
summary(College)

```

9.a.I split the data into training and test sets.
```{r ch_6_q_9_a}
#Split into training and test set
set.seed(1)
train=sample (1: nrow(College), nrow(College)/2)
test=-train
traincol = College[train, ]
testcol = College[test, ]
```

9.b. A linear model was fit on the training set and the test error ended up being 1108531.
```{r ch_6_q_9_b}


#Linear model using least squares on training set
lmappmodel = lm(Apps~., data=traincol)
model1pred = predict(lmappmodel, testcol)
mean((testcol[, "Apps"] - model1pred)^2)
summary(lmappmodel)
```

9.c.The lambda that results in the smallest cross-validation error is 0.01149757.The test error that results is 1108512. This is with set.seed(1).
```{r ch_6_q_9_c}
#Ridge regression model fit
library(glmnet)

#Set up x and y
x = model.matrix(Apps~., data=traincol)
y = model.matrix(Apps~., data=testcol)
grid = 10 ^ seq(4, -2, length=100)
mod.ridge = cv.glmnet(x, traincol[, "Apps"], alpha=0, lambda=grid, thresh=1e-12)
bestlam = mod.ridge$lambda.min
bestlam
summary(mod.ridge)

#Predict on test data
ridge.pred = predict(mod.ridge, newx=y, s=bestlam)
mean((testcol[, "Apps"] - ridge.pred)^2)

```

9.d.The lambda that was chosen is 28.48 and the test error was 1028718.The coefficients are in the coefficents output below and most are negative numbers.14 coefficients are non-zero.
```{r ch_6_q_9_d}
#Lasso model with lambda chosen by cross-validation
lasso.mod = cv.glmnet(x, traincol[, "Apps"], alpha=1, lambda=grid, thresh=1e-12)
bestlam = lasso.mod$lambda.min
bestlam
summary(lasso.mod)
#Predict on test data
predlasso = predict(lasso.mod, newx=y, s=bestlam)
mean((testcol[, "Apps"] - predlasso)^2)

#Coefficients
lasso = glmnet(model.matrix(Apps~., data=College), College[, "Apps"], alpha=1)
predict(lasso, s=bestlam, type="coefficients")
```

9.e.The test error is 1505718 for the PCR model. The M is 10 because that is the lowest cross-validation error.
```{r ch_6_q_9_e}
#PCR model with cross-validation chosen M
library(pls)
set.seed(1)
pcr.fit = pcr(Apps~., data=traincol, scale=T, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
summary(pcr.fit)
#Compute test error
pcr.pred = predict(pcr.fit, testcol, ncomp=10)
mean((testcol[, "Apps"] - data.frame(pcr.pred))^2)
```

9.f.The  M is 10 because it has the lowest cross-validation error.The test error is 1134531.
```{r ch_6_q_9_f}
#PLS model with cross-validation chosen M
pls.model = plsr(Apps~., data=traincol, scale=T, validation="CV")
validationplot(pls.model, val.type="MSEP")
summary(pls.model)
#predict test error.
pls.pred = predict(pls.model, testcol, ncomp=10)
mean((testcol[, "Apps"] - data.frame(pls.pred))^2)
```

9.g.The test errors were fairly similar in each of the model types. OLS and Ridge had the closest similar test errors with 1108531 and 1108512 respectively. Lasso and PLS were a little lower and higher than those two respectively with 1028718 and 1134531 test errors. The PCR test error was the most different from the other models with 1505718 being around 400 units higher.I looked at test R^2 for the models to see with how much accuracy predictions for college applications could be made.OLS,ridge, lasso, and pls all had higher accuracy being at or above 0.9(0.94,0.90,0.91,0.90 respectively). PCR accuracy was the lowest at 0.87. These are all fairly high accuracy numbers though.We can accurately predict the number of applications in the 0.9/90% range based on all the models' r^2 values.
```{r ch_6_q_9_g}
#Calculate R^2 for determining accuracy in predicting
avg = mean(testcol[, "Apps"])
ridger2 = 1 - mean((testcol[, "Apps"] - ridge.pred)^2) /mean((testcol[, "Apps"] - avg)^2)
lassor2 = 1 - mean((testcol[, "Apps"] - predlasso)^2) /mean((testcol[, "Apps"] - avg)^2)
pcrr2 = 1 - mean((testcol[, "Apps"] - data.frame(pcr.pred))^2) /mean((testcol[, "Apps"] - avg)^2)
plsr2 = 1 - mean((testcol[, "Apps"] - data.frame(pls.pred))^2) /mean((testcol[, "Apps"] - avg)^2)

c(ridger2,lassor2,pcrr2,plsr2)
```


11.
11.a. I made models using best subsets, lasso, ridge regression, and PCR.In the best subsets models the one that is best is the 9 variable model.6.59 is the cross-validated mean squared error for the best subsets model.Most of the coefficient estimates are zero in the Lasso model. The cross-validated  RMSE is 7.42 which is higher than the best subsets.There is no variable selection in the ridge regression model because none of the coefficients are zero. The cross-validated RMSE is 7.61.In the PCR model the model with the best/lowest CV/adjCV has 13 components and the cross-validated RMSE was 6.54.These are the various models that were explored.
```{r ch_6_q_11_g_best}
#Best subsets
set.seed(1)
sum(is.na(Boston$crim))


predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    mat[, names(coefi)] %*% coefi
}

k = 10
p = ncol(Boston) - 1
folds = sample(rep(1:k, length = nrow(Boston)))
cv.errors = matrix(NA, k, p)
for (i in 1:k) {
    best.fit = regsubsets(crim ~ ., data = Boston[folds != i, ], nvmax = p)
    for (j in 1:p) {
        pred = predict(best.fit, Boston[folds == i, ], id = j)
        cv.errors[i, j] = mean((Boston$crim[folds == i] - pred)^2)
    }
}
rmse.cv = sqrt(apply(cv.errors, 2, mean))
plot(rmse.cv, pch = 19, type = "b")

which.min(rmse.cv)
rmse.cv[which.min(rmse.cv)]

```

```{r ch_6_q_11_g_lasso}
#Lasso model
mod.mat = model.matrix(crim ~ . - 1, data = Boston)
crimmod= Boston$crim
lassocv = cv.glmnet(mod.mat, crimmod, type.measure = "mse")
plot(lassocv)
coef(lassocv)
sqrt(lassocv$cvm[lassocv$lambda == lassocv$lambda.1se])
```

```{r ch_6_q_11_g_ridge}
#Ridge Regression model
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
ridgecv2 = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
plot(ridgecv2)
coef(ridgecv2)
sqrt(ridgecv2$cvm[ridgecv2$lambda == ridgecv2$lambda.1se])
```

```{r ch_6_q_11_g_pcr}
#PCR model
pcr.fit2 = pcr(crim ~ ., data = Boston, scale = TRUE, validation = "CV")
summary(pcr.fit2)
```

11.b. The model that seemed the best out of the various methods would be the best subsets model. One reason is because it has a better cross-validated RMSE than the Lasso or Ridge models(Best subset:6.59, Lasso:7.42, Ridge:7.61) due to its being smaller.The PCR model had a better cross-validated RMSE than the best subset(6.54 vs. 6.59) but there were many more variables in this model(13 to the 9 in best subsets). In a model, simplicity and less variables is preferred. So this made me conclude that the best subsets model was the best model in this situation and seemed to perform well.These were all evaluated using cross-validation mean squared errors.

11.c.My chosen model(the best subsets model) does not involve all the features of the dataset.It only includes 9 parameters/variables from the dataset. This was chosen because it had the lowest cross-validated RMSE besides the PCR model. But the PCR model was more complex with 13 components. Model simplicity is preferred when the two RMSE's are so similar. That is why some features from the dataset are missing because it results in a better predictive model.

Chapter 7 problems(Ch.7 exercise 9) moved to Hw 3 according to email.